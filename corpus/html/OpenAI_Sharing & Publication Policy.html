
         Contents <ol>
<li>Social media, livestreaming, and demonstrations&nbsp;policy</li>
<li>Content co-authored with the OpenAI API&nbsp;policy</li>
<li>Research&nbsp;policy</li>
</ol> Social media, livestreaming, and demonstrations policy <p>To mitigate the possible risks of AI-generated content, we have set the following policy on permitted&nbsp;sharing.</p>
<p>
<strong>Posting your own prompts / completions to social media is generally permissible, as is livestreaming your usage or demonstrating our products to groups of people.
Please adhere to the&nbsp;following:</strong>
</p>
<ul>
<li>Manually review each generation before sharing or while&nbsp;streaming.</li>
<li>Attribute the content to your name or your&nbsp;company.</li>
<li>Indicate that the content is AI-generated in a way no user could reasonably miss or&nbsp;misunderstand.</li>
<li>Do not share content that violates our Content Policy or that may offend&nbsp;others.</li>
<li>If taking audience requests for prompts, use good judgment.
do not input prompts that might result in violations of our Content&nbsp;Policy.</li>
</ul>
<p>If you would like to ensure the OpenAI team is aware of a particular completion, you may email us or use the reporting tools within&nbsp;Playground.</p>
<ul>
<li>Recall that you are interacting with the raw model, which means we do not filter out biased or negative responses.
(Also, you can read more about implementing our free content filter&nbsp;here.)</li>
</ul>
<p>If you are building an application related to social media, note that you must be approved through an App Review before&nbsp;commencing.</p>
<ul>
<li>You may demonstrate your ideas prior to approval in venues such as hackathons, conferences, meetings with investors or coworkers, etc., so long as you indicate that your application has not yet been approved for&nbsp;launch.</li>
</ul> Content co-authored with the OpenAI API policy <p>Creators who wish to publish their first-party written content (e.g., a book, compendium of short stories) created in part with the OpenAI API are permitted to do so under the following&nbsp;conditions:</p>
<ul>
<li>The published content is attributed to your name or&nbsp;company.</li>
<li>The role of AI in formulating the content is clearly disclosed in a way that no reader could possibly miss, and that a typical reader would find sufficiently easy to&nbsp;understand.</li>
<li>Topics of the content do not violate OpenAI’s Content Policy or Terms of Use, e.g., are not related to political campaigns, adult content, spam, hateful content, content that incites violence, or other uses that may cause social&nbsp;harm.</li>
<li>We kindly ask that you refrain from sharing outputs that may offend&nbsp;others.</li>
</ul>
<p>For instance, one must detail in a Foreword or Introduction (or some place similar) the relative roles of drafting, editing, etc.
People should not represent API-generated content as being wholly generated by a human or wholly generated by an AI, and it is a human who must take ultimate responsibility for the content being&nbsp;published.</p>
<p>Here is some stock language you may use to describe your creative process, provided it is&nbsp;accurate:</p>
<p>
<em>“The author generated this text in part with GPT-3, OpenAI’s large-scale language-generation model.
Upon generating draft language, the author reviewed, edited, and revised the language to their own liking and takes ultimate responsibility for the content of this&nbsp;publication.”</em>
</p> Research policy <p>We believe it is important for the broader world to be able to evaluate our research and products, especially to understand and improve potential weaknesses and safety or bias problems in our models.
Accordingly, we welcome research publications related to the OpenAI&nbsp;API.</p>
<p>If you have any questions about research publications based on API access or would like to give us advanced notice of a publication (though not required), please email us at&nbsp;papers@openai.com.</p>
<ul>
<li>In some cases, we may want to highlight your work internally and/or&nbsp;externally.</li>
<li>In others, such as publications that pertain to security or misuse of the API, we may want to take appropriate actions to protect our&nbsp;users.</li>
<li>If you notice any safety or security issues with the API in the course of your research, we ask that you please submit these immediately through our Coordinated Vulnerability Disclosure&nbsp;Program.</li>
</ul>
<p>
<em>Researcher Access&nbsp;Program</em>
</p>
<p>There are a number of research directions we are excited to explore with the OpenAI API.
If you are interested in the opportunity for subsidized access, please provide us with details about your research use case using this&nbsp;form.</p>
<p>In particular, we consider the following to be especially important directions, though you are free to craft your own&nbsp;direction:</p>
<ul>
<li>
<strong>Alignment</strong>: How can we understand what objective, if any, a model is best understood as pursuing? How do we increase the extent to which that objective is aligned with human preferences, such as via prompt design or&nbsp;fine-tuning?</li>
<li>
<strong>Fairness and Representation</strong>: How should performance criteria be established for fairness and representation in language models? How can language models be improved in order to effectively support the goals of fairness and representation in specific, deployed&nbsp;contexts?</li>
<li>
<strong>Interdisciplinary Research</strong>: How can AI development draw on insights from other disciplines such as philosophy, cognitive science, and&nbsp;sociolinguistics?</li>
<li>
<strong>Interpretability / Transparency</strong>: How do these models work, mechanistically? Can we identify what concepts they’re using, or extract latent knowledge from the model, make inferences about the training procedure, or predict surprising future&nbsp;behavior?</li>
<li>
<strong>Misuse Potential</strong>: How can systems like the API be misused? What sorts of ‘red teaming’ approaches can we develop to help us and other AI developers think about responsibly deploying technologies like&nbsp;this?</li>
<li>
<strong>Model Exploration</strong>: Models like those served by the API have a variety of capabilities which we have yet to explore.
We’re excited by investigations in many areas including model limitations, linguistic properties, commonsense reasoning, and potential uses for many other&nbsp;problems.</li>
<li>
<strong>Robustness</strong>: Generative models have uneven capability surfaces, with the potential for surprisingly strong and surprisingly weak areas of capability.
How robust are large generative models to “natural” perturbations in the prompt, such as phrasing the same idea in different ways or with/without typos? Can we predict the kinds of domains and tasks for which large generative models are more likely to be robust (or not robust), and how does this relate to the training data? Are there techniques we can use to predict and mitigate worst-case behavior? How can robustness be measured in the context of few-shot learning (e.g.
across variations in prompts)? Can we train models so that they satisfy safety properties with a very high level of reliability, even under adversarial&nbsp;inputs?</li>
</ul>
<p>Please note that due to a high volume of requests, it takes time for us to review these applications and not all research will be prioritized for subsidy.
We will only be in touch if your application is selected for&nbsp;subsidy.</p> 
      